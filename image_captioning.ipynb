{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from string import digits\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "Get dataset and split into individual sentences\n",
    "\n",
    "Create word corpus (vocabulary list from dataset)\n",
    "\n",
    "Calculate total number of unique words, +1 for the padding \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/captions.txt') as f:\n",
    "    captions = [lines.strip().split(',') for lines in f.readlines()]\n",
    "\n",
    "captions = captions[1:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                   1     2     3     4     5  \\\n",
       "0  A child in a pink dress is climbing up a set o...  None  None  None  None   \n",
       "\n",
       "      6  \n",
       "0  None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = pd.DataFrame(captions)\n",
    "captions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455</td>\n",
       "      <td>40455</td>\n",
       "      <td>2447</td>\n",
       "      <td>715</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8091</td>\n",
       "      <td>39929</td>\n",
       "      <td>2288</td>\n",
       "      <td>705</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1285067106_2adc307240.jpg</td>\n",
       "      <td>\"Two dogs</td>\n",
       "      <td>\"</td>\n",
       "      <td>and a backpack .\"</td>\n",
       "      <td>are in a mall .\"</td>\n",
       "      <td>cover their mouths .\"</td>\n",
       "      <td>are waving to the crowd along the parade rout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0           1     2                   3  \\\n",
       "count                       40455       40455  2447                 715   \n",
       "unique                       8091       39929  2288                 705   \n",
       "top     1285067106_2adc307240.jpg  \"Two dogs      \"   and a backpack .\"   \n",
       "freq                            5          37    19                   3   \n",
       "\n",
       "                        4                       5  \\\n",
       "count                  64                       5   \n",
       "unique                 64                       5   \n",
       "top      are in a mall .\"   cover their mouths .\"   \n",
       "freq                    1                       1   \n",
       "\n",
       "                                                        6  \n",
       "count                                                   1  \n",
       "unique                                                  1  \n",
       "top      are waving to the crowd along the parade rout...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing For The Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_lines_split = []\n",
    "# for i in [2,3,4,5,6]:\n",
    "#     idx_lines_split += list(captions[captions[i].notnull()].index)\n",
    "\n",
    "# idx_lines_split = list(set(idx_lines_split))\n",
    "\n",
    "# remove_digits = str.maketrans('', '', digits)\n",
    "# for index in idx_lines_split:\n",
    "#     # merging the sentences that were split into multiple columns into a string\n",
    "#     merged_caption = \" \".join(captions.iloc[index,[1,2,3,4,5,6]].fillna(\"\").values).strip()\n",
    "#     captions.iloc[index,1] = merged_caption\n",
    "\n",
    "# cleaned_captions = []\n",
    "# for i in range(len(captions)):\n",
    "#     # removing numbers from the string\n",
    "#     cleaned_caption = captions.iloc[i,1].translate(remove_digits)\n",
    "#     cleaned_caption = \"starttoken \" + cleaned_caption + \" endtoken\"\n",
    "#     cleaned_captions.append(cleaned_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/cleaned_captions.pickle\", 'wb') as file_pi:\n",
    "#     pickle.dump(cleaned_captions, file_pi)\n",
    "\n",
    "with open(\"data/cleaned_captions.pickle\", \"rb\") as f:\n",
    "    cleaned_captions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_model = tf.keras.applications.Xception(\n",
    "#     weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "#     input_shape=(299, 299, 3),\n",
    "#     pooling='avg',\n",
    "#     include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # every 5 rows, extract the image linked to those 5 captions\n",
    "# img_features = []\n",
    "# for i in range(0,len(captions),5):\n",
    "#     # reading the file\n",
    "#     filename = captions.iloc[i,0]\n",
    "#     img = Image.open(\"data/Images/\" + filename)\n",
    "#     # resizing images to 299 by 299 as its the default model input size\n",
    "#     img = img.resize((299,299))\n",
    "#     img = np.array(img)\n",
    "#     # preprocess image pixel values to -1 to 1\n",
    "#     img = np.expand_dims(img, axis=0)\n",
    "#     img = tf.keras.applications.xception.preprocess_input(img)\n",
    "#     # extract feature vectors using the encoder CNN model\n",
    "#     features = encoder_model.predict(img)\n",
    "#     img_features.append(features)\n",
    "\n",
    "# with open(\"data/image_features.pickle\", 'wb') as file_pi:\n",
    "#     pickle.dump(img_features, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/image_features.pickle\", \"rb\") as x:\n",
    "    img_features = pickle.load(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(cleaned_captions)\n",
    "total_words = len(tokenizer.word_index) + 1 # +1 because of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "caption_lengths = []\n",
    "for line in cleaned_captions: # for each sentence in dataset\n",
    "\t# line = \"starttoken \" + cleaned_captions[i] + \" endtoken\"\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0] # tokenize each sentence\n",
    "\tnum_words = len(token_list)\n",
    "\tfor j in range(1, num_words): # create n-grams for each sentence so that\n",
    "\t\tn_gram_sequence = token_list[:j+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "\tcaption_lengths.append(num_words)\n",
    "\n",
    "# pad sequences so that inputs will have same length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label (last word in sentence)\n",
    "sequence, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=total_words) #convert them to categorical and one hot encode them so that it can predict which word from the vocab it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_img_features = []\n",
    "for i in range(0,len(img_features)):\n",
    "    # find out how many times each image must appear in the dataset to match the number of n-grams for each image\n",
    "    corresponding_caption_idx = i * 5\n",
    "    num_repeating_imgs = sum(caption_lengths[corresponding_caption_idx:corresponding_caption_idx+5]) - 5 # -5 because each caption has 1 less n-gram than words and there are 5 captions\n",
    "    for j in range(num_repeating_imgs):\n",
    "        training_img_features.append(img_features[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477906\n",
      "477906\n",
      "477906\n"
     ]
    }
   ],
   "source": [
    "print(len(training_img_features))\n",
    "print(len(sequence))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "    def __init__(self, total_words, input_length):\n",
    "        self.total_words = total_words\n",
    "        self.input_length = input_length\n",
    "    def build_encoder(self):\n",
    "        text_input = keras.Input(shape=(self.input_length,))\n",
    "\n",
    "        x = Embedding(self.total_words, 100, input_length=self.input_length)(text_input)\n",
    "        x = Bidirectional(LSTM(150))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1)) #embedding includes start token\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax')) # model shjld not predict start token so -1\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "history = model.fit(sequence, labels, epochs=100, verbose=1)\n",
    "#print model.summary()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_discriminator(self):\n",
    "        image_input = Input(shape=self.image_size)\n",
    "\n",
    "        x = self.conv_block(image_input, 128, 4, 2, 'same', use_batch_norm=True, use_bias=False, activation=LeakyReLU(alpha=leaky_relu_slope))\n",
    "        x = self.conv_block(x, 128, 4, 2, 'same', use_batch_norm=True, use_bias=False, activation=LeakyReLU(alpha=leaky_relu_slope))\n",
    "        x = self.conv_block(x, 128, 4, 2, 'same', use_batch_norm=True, use_bias=False, activation=LeakyReLU(alpha=leaky_relu_slope))\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(0.4)(x)\n",
    "        output_score = Dense(1)(x)\n",
    "\n",
    "        return keras.Model(image_input, output_score, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"I've got a bad feeling about this\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stuff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ee5403e18d804ebb44ee52a33f7027b543cc38948065ce95628afdc6386affb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
